{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5a937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4763d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for noise level 1...\n",
      "  Seed 1 done.\n",
      "  Seed 2 done.\n",
      "  Seed 3 done.\n",
      "  Seed 4 done.\n",
      "  Seed 5 done.\n",
      "  Seed 6 done.\n",
      "  Seed 7 done.\n",
      "  Seed 8 done.\n",
      "  Seed 9 done.\n",
      "  Seed 10 done.\n",
      "  Seed 11 done.\n",
      "  Seed 12 done.\n",
      "  Seed 13 done.\n",
      "  Seed 14 done.\n",
      "  Seed 15 done.\n",
      "  Seed 16 done.\n",
      "  Seed 17 done.\n",
      "  Seed 18 done.\n",
      "  Seed 19 done.\n",
      "  Seed 20 done.\n",
      "  Seed 21 done.\n",
      "  Seed 22 done.\n",
      "  Seed 23 done.\n",
      "  Seed 24 done.\n",
      "  Seed 25 done.\n",
      "  Seed 26 done.\n",
      "  Seed 27 done.\n",
      "  Seed 28 done.\n",
      "  Seed 29 done.\n",
      "  Seed 30 done.\n",
      "  Seed 31 done.\n",
      "  Seed 32 done.\n",
      "  Seed 33 done.\n",
      "  Seed 34 done.\n",
      "  Seed 35 done.\n",
      "  Seed 36 done.\n",
      "  Seed 37 done.\n",
      "  Seed 38 done.\n",
      "  Seed 39 done.\n",
      "  Seed 40 done.\n",
      "  Seed 41 done.\n",
      "  Seed 42 done.\n",
      "  Seed 43 done.\n",
      "  Seed 44 done.\n",
      "  Seed 45 done.\n",
      "  Seed 46 done.\n",
      "  Seed 47 done.\n",
      "  Seed 48 done.\n",
      "  Seed 49 done.\n",
      "  Seed 50 done.\n",
      "  Seed 51 done.\n",
      "  Seed 52 done.\n",
      "  Seed 53 done.\n",
      "  Seed 54 done.\n",
      "  Seed 55 done.\n",
      "  Seed 56 done.\n",
      "  Seed 57 done.\n",
      "  Seed 58 done.\n",
      "  Seed 59 done.\n",
      "  Seed 60 done.\n",
      "  Seed 61 done.\n",
      "  Seed 62 done.\n",
      "  Seed 63 done.\n",
      "  Seed 64 done.\n",
      "  Seed 65 done.\n",
      "  Seed 66 done.\n",
      "  Seed 67 done.\n",
      "  Seed 68 done.\n",
      "  Seed 69 done.\n",
      "  Seed 70 done.\n",
      "  Seed 71 done.\n",
      "  Seed 72 done.\n",
      "  Seed 73 done.\n",
      "  Seed 74 done.\n",
      "  Seed 75 done.\n",
      "  Seed 76 done.\n",
      "  Seed 77 done.\n",
      "  Seed 78 done.\n",
      "  Seed 79 done.\n",
      "  Seed 80 done.\n",
      "  Seed 81 done.\n",
      "  Seed 82 done.\n",
      "  Seed 83 done.\n",
      "  Seed 84 done.\n",
      "  Seed 85 done.\n",
      "  Seed 86 done.\n",
      "  Seed 87 done.\n",
      "  Seed 88 done.\n",
      "  Seed 89 done.\n",
      "  Seed 90 done.\n",
      "  Seed 91 done.\n",
      "  Seed 92 done.\n",
      "  Seed 93 done.\n",
      "  Seed 94 done.\n",
      "  Seed 95 done.\n",
      "  Seed 96 done.\n",
      "  Seed 97 done.\n",
      "  Seed 98 done.\n",
      "  Seed 99 done.\n",
      "  Seed 100 done.\n",
      "Running experiments for noise level 2...\n",
      "  Seed 1 done.\n",
      "  Seed 2 done.\n",
      "  Seed 3 done.\n",
      "  Seed 4 done.\n",
      "  Seed 5 done.\n",
      "  Seed 6 done.\n",
      "  Seed 7 done.\n",
      "  Seed 8 done.\n",
      "  Seed 9 done.\n",
      "  Seed 10 done.\n",
      "  Seed 11 done.\n",
      "  Seed 12 done.\n",
      "  Seed 13 done.\n",
      "  Seed 14 done.\n",
      "  Seed 15 done.\n",
      "  Seed 16 done.\n",
      "  Seed 17 done.\n",
      "  Seed 18 done.\n",
      "  Seed 19 done.\n",
      "  Seed 20 done.\n",
      "  Seed 21 done.\n",
      "  Seed 22 done.\n",
      "  Seed 23 done.\n",
      "  Seed 24 done.\n",
      "  Seed 25 done.\n",
      "  Seed 26 done.\n",
      "  Seed 27 done.\n",
      "  Seed 28 done.\n",
      "  Seed 29 done.\n",
      "  Seed 30 done.\n",
      "  Seed 31 done.\n",
      "  Seed 32 done.\n",
      "  Seed 33 done.\n",
      "  Seed 34 done.\n",
      "  Seed 35 done.\n",
      "  Seed 36 done.\n",
      "  Seed 37 done.\n",
      "  Seed 38 done.\n",
      "  Seed 39 done.\n",
      "  Seed 40 done.\n",
      "  Seed 41 done.\n",
      "  Seed 42 done.\n",
      "  Seed 43 done.\n",
      "  Seed 44 done.\n",
      "  Seed 45 done.\n",
      "  Seed 46 done.\n",
      "  Seed 47 done.\n",
      "  Seed 48 done.\n",
      "  Seed 49 done.\n",
      "  Seed 50 done.\n",
      "  Seed 51 done.\n",
      "  Seed 52 done.\n",
      "  Seed 53 done.\n",
      "  Seed 54 done.\n",
      "  Seed 55 done.\n",
      "  Seed 56 done.\n",
      "  Seed 57 done.\n",
      "  Seed 58 done.\n",
      "  Seed 59 done.\n",
      "  Seed 60 done.\n",
      "  Seed 61 done.\n",
      "  Seed 62 done.\n",
      "  Seed 63 done.\n",
      "  Seed 64 done.\n",
      "  Seed 65 done.\n",
      "  Seed 66 done.\n",
      "  Seed 67 done.\n",
      "  Seed 68 done.\n",
      "  Seed 69 done.\n",
      "  Seed 70 done.\n",
      "  Seed 71 done.\n",
      "  Seed 72 done.\n",
      "  Seed 73 done.\n",
      "  Seed 74 done.\n",
      "  Seed 75 done.\n",
      "  Seed 76 done.\n",
      "  Seed 77 done.\n",
      "  Seed 78 done.\n",
      "  Seed 79 done.\n",
      "  Seed 80 done.\n",
      "  Seed 81 done.\n",
      "  Seed 82 done.\n",
      "  Seed 83 done.\n",
      "  Seed 84 done.\n",
      "  Seed 85 done.\n",
      "  Seed 86 done.\n",
      "  Seed 87 done.\n",
      "  Seed 88 done.\n",
      "  Seed 89 done.\n",
      "  Seed 90 done.\n",
      "  Seed 91 done.\n",
      "  Seed 92 done.\n",
      "  Seed 93 done.\n",
      "  Seed 94 done.\n",
      "  Seed 95 done.\n",
      "  Seed 96 done.\n",
      "  Seed 97 done.\n",
      "  Seed 98 done.\n",
      "  Seed 99 done.\n",
      "  Seed 100 done.\n",
      "Running experiments for noise level 3...\n",
      "  Seed 1 done.\n",
      "  Seed 2 done.\n",
      "  Seed 3 done.\n",
      "  Seed 4 done.\n",
      "  Seed 5 done.\n",
      "  Seed 6 done.\n",
      "  Seed 7 done.\n",
      "  Seed 8 done.\n",
      "  Seed 9 done.\n",
      "  Seed 10 done.\n",
      "  Seed 11 done.\n",
      "  Seed 12 done.\n",
      "  Seed 13 done.\n",
      "  Seed 14 done.\n",
      "  Seed 15 done.\n",
      "  Seed 16 done.\n",
      "  Seed 17 done.\n",
      "  Seed 18 done.\n",
      "  Seed 19 done.\n",
      "  Seed 20 done.\n",
      "  Seed 21 done.\n",
      "  Seed 22 done.\n",
      "  Seed 23 done.\n",
      "  Seed 24 done.\n",
      "  Seed 25 done.\n",
      "  Seed 26 done.\n",
      "  Seed 27 done.\n",
      "  Seed 28 done.\n",
      "  Seed 29 done.\n",
      "  Seed 30 done.\n",
      "  Seed 31 done.\n",
      "  Seed 32 done.\n",
      "  Seed 33 done.\n",
      "  Seed 34 done.\n",
      "  Seed 35 done.\n",
      "  Seed 36 done.\n",
      "  Seed 37 done.\n",
      "  Seed 38 done.\n",
      "  Seed 39 done.\n",
      "  Seed 40 done.\n",
      "  Seed 41 done.\n",
      "  Seed 42 done.\n",
      "  Seed 43 done.\n",
      "  Seed 44 done.\n",
      "  Seed 45 done.\n",
      "  Seed 46 done.\n",
      "  Seed 47 done.\n",
      "  Seed 48 done.\n",
      "  Seed 49 done.\n",
      "  Seed 50 done.\n",
      "  Seed 51 done.\n",
      "  Seed 52 done.\n",
      "  Seed 53 done.\n",
      "  Seed 54 done.\n",
      "  Seed 55 done.\n",
      "  Seed 56 done.\n",
      "  Seed 57 done.\n",
      "  Seed 58 done.\n",
      "  Seed 59 done.\n",
      "  Seed 60 done.\n",
      "  Seed 61 done.\n",
      "  Seed 62 done.\n",
      "  Seed 63 done.\n",
      "  Seed 64 done.\n",
      "  Seed 65 done.\n",
      "  Seed 66 done.\n",
      "  Seed 67 done.\n",
      "  Seed 68 done.\n",
      "  Seed 69 done.\n",
      "  Seed 70 done.\n",
      "  Seed 71 done.\n",
      "  Seed 72 done.\n",
      "  Seed 73 done.\n",
      "  Seed 74 done.\n",
      "  Seed 75 done.\n",
      "  Seed 76 done.\n",
      "  Seed 77 done.\n",
      "  Seed 78 done.\n",
      "  Seed 79 done.\n",
      "  Seed 80 done.\n",
      "  Seed 81 done.\n",
      "  Seed 82 done.\n",
      "  Seed 83 done.\n",
      "  Seed 84 done.\n",
      "  Seed 85 done.\n",
      "  Seed 86 done.\n",
      "  Seed 87 done.\n",
      "  Seed 88 done.\n",
      "  Seed 89 done.\n",
      "  Seed 90 done.\n",
      "  Seed 91 done.\n",
      "  Seed 92 done.\n",
      "  Seed 93 done.\n",
      "  Seed 94 done.\n",
      "  Seed 95 done.\n",
      "  Seed 96 done.\n",
      "  Seed 97 done.\n",
      "  Seed 98 done.\n",
      "  Seed 99 done.\n",
      "  Seed 100 done.\n",
      "Running experiments for noise level 5...\n",
      "  Seed 1 done.\n",
      "  Seed 2 done.\n",
      "  Seed 3 done.\n",
      "  Seed 4 done.\n",
      "  Seed 5 done.\n",
      "  Seed 6 done.\n",
      "  Seed 7 done.\n",
      "  Seed 8 done.\n",
      "  Seed 9 done.\n",
      "  Seed 10 done.\n",
      "  Seed 11 done.\n",
      "  Seed 12 done.\n",
      "  Seed 13 done.\n",
      "  Seed 14 done.\n",
      "  Seed 15 done.\n",
      "  Seed 16 done.\n",
      "  Seed 17 done.\n",
      "  Seed 18 done.\n",
      "  Seed 19 done.\n",
      "  Seed 20 done.\n",
      "  Seed 21 done.\n",
      "  Seed 22 done.\n",
      "  Seed 23 done.\n",
      "  Seed 24 done.\n",
      "  Seed 25 done.\n",
      "  Seed 26 done.\n",
      "  Seed 27 done.\n",
      "  Seed 28 done.\n",
      "  Seed 29 done.\n",
      "  Seed 30 done.\n",
      "  Seed 31 done.\n",
      "  Seed 32 done.\n",
      "  Seed 33 done.\n",
      "  Seed 34 done.\n",
      "  Seed 35 done.\n",
      "  Seed 36 done.\n",
      "  Seed 37 done.\n",
      "  Seed 38 done.\n",
      "  Seed 39 done.\n",
      "  Seed 40 done.\n",
      "  Seed 41 done.\n",
      "  Seed 42 done.\n",
      "  Seed 43 done.\n",
      "  Seed 44 done.\n",
      "  Seed 45 done.\n",
      "  Seed 46 done.\n",
      "  Seed 47 done.\n",
      "  Seed 48 done.\n",
      "  Seed 49 done.\n",
      "  Seed 50 done.\n",
      "  Seed 51 done.\n",
      "  Seed 52 done.\n",
      "  Seed 53 done.\n",
      "  Seed 54 done.\n",
      "  Seed 55 done.\n",
      "  Seed 56 done.\n",
      "  Seed 57 done.\n",
      "  Seed 58 done.\n",
      "  Seed 59 done.\n",
      "  Seed 60 done.\n",
      "  Seed 61 done.\n",
      "  Seed 62 done.\n",
      "  Seed 63 done.\n",
      "  Seed 64 done.\n",
      "  Seed 65 done.\n",
      "  Seed 66 done.\n",
      "  Seed 67 done.\n",
      "  Seed 68 done.\n",
      "  Seed 69 done.\n",
      "  Seed 70 done.\n",
      "  Seed 71 done.\n",
      "  Seed 72 done.\n",
      "  Seed 73 done.\n",
      "  Seed 74 done.\n",
      "  Seed 75 done.\n",
      "  Seed 76 done.\n",
      "  Seed 77 done.\n",
      "  Seed 78 done.\n",
      "  Seed 79 done.\n",
      "  Seed 80 done.\n",
      "  Seed 81 done.\n",
      "  Seed 82 done.\n",
      "  Seed 83 done.\n",
      "  Seed 84 done.\n",
      "  Seed 85 done.\n",
      "  Seed 86 done.\n",
      "  Seed 87 done.\n",
      "  Seed 88 done.\n",
      "  Seed 89 done.\n",
      "  Seed 90 done.\n",
      "  Seed 91 done.\n",
      "  Seed 92 done.\n",
      "  Seed 93 done.\n",
      "  Seed 94 done.\n",
      "  Seed 95 done.\n",
      "  Seed 96 done.\n",
      "  Seed 97 done.\n",
      "  Seed 98 done.\n",
      "  Seed 99 done.\n",
      "  Seed 100 done.\n",
      "\n",
      "✅ All results saved to C:\\Document\\Serieux\\Travail\\python_work\\cEBMF_additional_simulation_VAE\\tiling_benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ----------------------------- Data Generation ----------------------------- #\n",
    "def create_data(noise_level, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    x = np.random.rand(1000)\n",
    "    y = np.random.rand(1000)\n",
    "    X = np.vstack((x, y)).T\n",
    "\n",
    "    f = np.zeros((3, 200))\n",
    "    for i in range(f.shape[1]):\n",
    "        t1, t2 = np.random.randint(2), np.random.randint(2)\n",
    "        f[0, i] = t1 * np.random.randn()\n",
    "        f[1, i] = t2 * np.random.randn()\n",
    "        f[2, i] = t2 * np.random.randn()\n",
    "\n",
    "    L = np.zeros((len(x), 3))\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i] < 0.33 and y[i] < 0.33) or\n",
    "            (0.33 < x[i] < 0.66 and 0.33 < y[i] < 0.66) or\n",
    "            (x[i] > 0.66 and y[i] > 0.66)):\n",
    "            L[i] = [1, 0, 0]\n",
    "        elif ((x[i] < 0.33 and y[i] > 0.66) or\n",
    "              (0.33 < x[i] < 0.66 and y[i] < 0.33) or\n",
    "              (x[i] > 0.66 and 0.33 < y[i] < 0.66)):\n",
    "            L[i] = [0, 1, 0]\n",
    "        else:\n",
    "            L[i] = [0, 0, 1]\n",
    "\n",
    "    Y_true = L @ f\n",
    "    Z = Y_true + np.random.randn(*Y_true.shape) * noise_level\n",
    "    return X.astype(np.float32), Z.astype(np.float32), Y_true.astype(np.float32)\n",
    "\n",
    "# ----------------------------- Models ----------------------------- #\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 64), nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat([x, c], dim=1))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(torch.cat([z, c], dim=1))\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "class DenoisingNCF(nn.Module):\n",
    "    def __init__(self, input_dim, side_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(side_dim, hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_noisy, x_side):\n",
    "        u = self.user_mlp(x_side)\n",
    "        i = self.item_mlp(z_noisy)\n",
    "        return self.predictor(torch.cat([u, i], dim=1))\n",
    "\n",
    "# ----------------------------- Training Functions ----------------------------- #\n",
    "def train_ae(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch in dataloader:\n",
    "            x_batch = x_batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = model(x_batch)\n",
    "            loss = loss_fn(x_hat, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_cvae(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, c_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, mu, logvar = model(x_batch, c_batch)\n",
    "            recon_loss = nn.functional.mse_loss(x_hat, x_batch)\n",
    "            kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_div\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_ncf(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for z_batch, x_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            z_hat = model(z_batch, x_batch)\n",
    "            loss = loss_fn(z_hat, z_batch)  # Predict noisy Z\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# ----------------------------- Benchmark Routine ----------------------------- #\n",
    "def run_denoising_benchmark(noise_level=1.0, seed=1):\n",
    "    X, Z, Y_true = create_data(noise_level, seed)\n",
    "    batch_size = 32\n",
    "\n",
    "    tensor_X = torch.tensor(X)\n",
    "    tensor_Z = torch.tensor(Z)\n",
    "\n",
    "    # AE\n",
    "    ae = AE(input_dim=Z.shape[1], latent_dim=10)\n",
    "    loader_ae = DataLoader(TensorDataset(tensor_Z), batch_size=batch_size, shuffle=True)\n",
    "    train_ae(ae, loader_ae)\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_ae = ae(tensor_Z).numpy()\n",
    "\n",
    "    # CVAE\n",
    "    cvae = CVAE(input_dim=Z.shape[1], cond_dim=2, latent_dim=10)\n",
    "    loader_cvae = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "    train_cvae(cvae, loader_cvae)\n",
    "    cvae.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_cvae, _, _ = cvae(tensor_Z, tensor_X)\n",
    "        Z_cvae = Z_cvae.numpy()\n",
    "\n",
    "    # NCF\n",
    "    ncf = DenoisingNCF(input_dim=Z.shape[1], side_dim=X.shape[1])\n",
    "    loader_ncf = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "    train_ncf(ncf, loader_ncf)\n",
    "    ncf.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_ncf = ncf(tensor_Z, tensor_X).numpy()\n",
    "\n",
    "    rmse = lambda A, B: np.sqrt(mean_squared_error(A, B))\n",
    "    return {\n",
    "        \"AE\": rmse(Z_ae, Y_true),\n",
    "        \"CVAE\": rmse(Z_cvae, Y_true),\n",
    "        \"NCF\": rmse(Z_ncf, Y_true),\n",
    "        \"noise_level\": noise_level,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "# ----------------------------- Main Loop ----------------------------- #\n",
    "output_file = r\"C:\\Document\\Serieux\\Travail\\python_work\\cEBMF_additional_simulation_VAE\\tiling_benchmark_results.csv\"\n",
    "all_results = []\n",
    "\n",
    "for noise in [1, 2, 3, 5]:\n",
    "    print(f\"Running experiments for noise level {noise}...\")\n",
    "    for i in range(100):\n",
    "        result = run_denoising_benchmark(noise_level=noise, seed=i + 1)\n",
    "        all_results.append(result)\n",
    "        print(f\"  Seed {i+1} done.\")\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ All results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ----------------------------- Data Generation ----------------------------- #\n",
    "def create_data(noise_level, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    x = np.random.rand(1000)\n",
    "    y = np.random.rand(1000)\n",
    "    X = np.vstack((x, y)).T\n",
    "\n",
    "    f = np.zeros((3, 200))\n",
    "    for i in range(f.shape[1]):\n",
    "        t1, t2 = np.random.randint(2), np.random.randint(2)\n",
    "        f[0, i] = t1 * np.random.randn()\n",
    "        f[1, i] = t2 * np.random.randn()\n",
    "        f[2, i] = t2 * np.random.randn()\n",
    "\n",
    "    L = np.zeros((len(x), 3))\n",
    "    for i in range(len(x)):\n",
    "        if ((x[i] < 0.33 and y[i] < 0.33) or\n",
    "            (0.33 < x[i] < 0.66 and 0.33 < y[i] < 0.66) or\n",
    "            (x[i] > 0.66 and y[i] > 0.66)):\n",
    "            L[i] = [1, 0, 0]\n",
    "        elif ((x[i] < 0.33 and y[i] > 0.66) or\n",
    "              (0.33 < x[i] < 0.66 and y[i] < 0.33) or\n",
    "              (x[i] > 0.66 and 0.33 < y[i] < 0.66)):\n",
    "            L[i] = [0, 1, 0]\n",
    "        else:\n",
    "            L[i] = [0, 0, 1]\n",
    "\n",
    "    Y_true = L @ f\n",
    "    Z = Y_true + np.random.randn(*Y_true.shape) * noise_level\n",
    "    return X.astype(np.float32), Z.astype(np.float32), Y_true.astype(np.float32)\n",
    "\n",
    "# ----------------------------- Models ----------------------------- #\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 64), nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat([x, c], dim=1))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(torch.cat([z, c], dim=1))\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "class DenoisingNCF(nn.Module):\n",
    "    def __init__(self, input_dim, side_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(side_dim, hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_noisy, x_side):\n",
    "        u = self.user_mlp(x_side)\n",
    "        i = self.item_mlp(z_noisy)\n",
    "        return self.predictor(torch.cat([u, i], dim=1))\n",
    "\n",
    "# ----------------------------- Training Functions ----------------------------- #\n",
    "def train_ae(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch in dataloader:\n",
    "            x_batch = x_batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = model(x_batch)\n",
    "            loss = loss_fn(x_hat, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_cvae(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, c_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, mu, logvar = model(x_batch, c_batch)\n",
    "            recon_loss = nn.functional.mse_loss(x_hat, x_batch)\n",
    "            kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_div\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_ncf(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for z_batch, x_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            z_hat = model(z_batch, x_batch)\n",
    "            loss = loss_fn(z_hat, z_batch)  # Predict noisy Z\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# ----------------------------- Benchmark Routine ----------------------------- #\n",
    "def run_denoising_benchmark(noise_level=1.0, seed=1):\n",
    "    X, Z, Y_true = create_data(noise_level, seed)\n",
    "    batch_size = 32\n",
    "\n",
    "    tensor_X = torch.tensor(X)\n",
    "    tensor_Z = torch.tensor(Z)\n",
    "\n",
    "    # AE\n",
    "    ae = AE(input_dim=Z.shape[1], latent_dim=10)\n",
    "    loader_ae = DataLoader(TensorDataset(tensor_Z), batch_size=batch_size, shuffle=True)\n",
    "    train_ae(ae, loader_ae)\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_ae = ae(tensor_Z).numpy()\n",
    "\n",
    "    # CVAE\n",
    "    cvae = CVAE(input_dim=Z.shape[1], cond_dim=2, latent_dim=10)\n",
    "    loader_cvae = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "    train_cvae(cvae, loader_cvae)\n",
    "    cvae.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_cvae, _, _ = cvae(tensor_Z, tensor_X)\n",
    "        Z_cvae = Z_cvae.numpy()\n",
    "\n",
    "    # NCF\n",
    "    ncf = DenoisingNCF(input_dim=Z.shape[1], side_dim=X.shape[1])\n",
    "    loader_ncf = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "    train_ncf(ncf, loader_ncf)\n",
    "    ncf.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_ncf = ncf(tensor_Z, tensor_X).numpy()\n",
    "\n",
    "    rmse = lambda A, B: np.sqrt(mean_squared_error(A, B))\n",
    "    return {\n",
    "        \"AE\": rmse(Z_ae, Y_true),\n",
    "        \"CVAE\": rmse(Z_cvae, Y_true),\n",
    "        \"NCF\": rmse(Z_ncf, Y_true),\n",
    "        \"noise_level\": noise_level,\n",
    "        \"seed\": seed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc586dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Z, Y_true = create_data(1, 1)\n",
    "batch_size = 32\n",
    "\n",
    "tensor_X = torch.tensor(X)\n",
    "tensor_Z = torch.tensor(Z)\n",
    "\n",
    "    # AE\n",
    "ae = AE(input_dim=Z.shape[1], latent_dim=10)\n",
    "loader_ae = DataLoader(TensorDataset(tensor_Z), batch_size=batch_size, shuffle=True)\n",
    "train_ae(ae, loader_ae)\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "        Z_ae = ae(tensor_Z).numpy()\n",
    "\n",
    "    # CVAE\n",
    "cvae = CVAE(input_dim=Z.shape[1], cond_dim=2, latent_dim=10)\n",
    "loader_cvae = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "train_cvae(cvae, loader_cvae)\n",
    "cvae.eval()\n",
    "with torch.no_grad():\n",
    "        Z_cvae, _, _ = cvae(tensor_Z, tensor_X)\n",
    "        Z_cvae = Z_cvae.numpy()\n",
    "\n",
    "    # NCF\n",
    "ncf = DenoisingNCF(input_dim=Z.shape[1], side_dim=X.shape[1])\n",
    "loader_ncf = DataLoader(TensorDataset(tensor_Z, tensor_X), batch_size=batch_size, shuffle=True)\n",
    "train_ncf(ncf, loader_ncf)\n",
    "ncf.eval()\n",
    "with torch.no_grad():\n",
    "        Z_ncf = ncf(tensor_Z, tensor_X).numpy()\n",
    "\n",
    "rmse = lambda A, B: np.sqrt(mean_squared_error(A, B))\n",
    " \n",
    "rmse(Z_ncf, Y_true),\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
